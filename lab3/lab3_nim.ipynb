{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Copyright **`(c)`** 2022 Giovanni Squillero `<squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Lab 3: Policy Search\n",
    "\n",
    "## Task\n",
    "\n",
    "Write agents able to play [*Nim*](https://en.wikipedia.org/wiki/Nim), with an arbitrary number of rows and an upper bound $k$ on the number of objects that can be removed in a turn (a.k.a., *subtraction game*).\n",
    "\n",
    "The player **taking the last object wins**.\n",
    "\n",
    "* Task3.1: An agent using fixed rules based on *nim-sum* (i.e., an *expert system*)\n",
    "* Task3.2: An agent using evolved rules\n",
    "* Task3.3: An agent using minmax\n",
    "* Task3.4: An agent using reinforcement learning\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Create the directory `lab3` inside the course repo \n",
    "* Put a `README.md` and your solution (all the files, code and auxiliary data if needed)\n",
    "\n",
    "## Notes\n",
    "\n",
    "* Working in group is not only allowed, but recommended (see: [Ubuntu](https://en.wikipedia.org/wiki/Ubuntu_philosophy) and [Cooperative Learning](https://files.eric.ed.gov/fulltext/EJ1096789.pdf)). Collaborations must be explicitly declared in the `README.md`.\n",
    "* [Yanking](https://www.emacswiki.org/emacs/KillingAndYanking) from the internet is allowed, but sources must be explicitly declared in the `README.md`.\n",
    "\n",
    "## Deadlines ([AoE](https://en.wikipedia.org/wiki/Anywhere_on_Earth))\n",
    "\n",
    "* Sunday, December 4th for Task3.1 and Task3.2\n",
    "* Sunday, December 11th for Task3.3 and Task3.4\n",
    "* Sunday, December 18th for all reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections import namedtuple\n",
    "import random\n",
    "from typing import Callable\n",
    "from copy import deepcopy\n",
    "from itertools import accumulate, product\n",
    "from operator import xor\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The *Nim* and *Nimply* classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nimply = namedtuple(\"Nimply\", \"row, num_objects\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nim:\n",
    "    def __init__(self, num_rows: int, k: int = None) -> None:\n",
    "        self._rows = [i * 2 + 1 for i in range(num_rows)]\n",
    "        self._k = k\n",
    "\n",
    "    def __bool__(self):\n",
    "        return sum(self._rows) > 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<\" + \" \".join(str(_) for _ in self._rows) + \">\"\n",
    "\n",
    "    def assign_rows(self, rows):\n",
    "        self._rows = list(rows)\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def rows(self) -> tuple:\n",
    "        return tuple(self._rows)\n",
    "\n",
    "    @property\n",
    "    def k(self) -> int:\n",
    "        return self._k\n",
    "\n",
    "    def nimming(self, ply: Nimply) -> None:\n",
    "        row, num_objects = ply\n",
    "        assert self._rows[row] >= num_objects\n",
    "        assert self._k is None or num_objects <= self._k\n",
    "        self._rows[row] -= num_objects\n",
    "\n",
    "    def game_over(self) -> bool:\n",
    "        return sum(self._rows) == 0\n",
    "        \n",
    "def active_rows(state: Nim) -> int:\n",
    "    return sum(o > 0 for o in state.rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nim_sum(state: Nim) -> int:\n",
    "    *_, result = accumulate(state.rows, xor)\n",
    "    return result\n",
    "\n",
    "\n",
    "def cook_status(state: Nim) -> dict:\n",
    "    cooked = dict()\n",
    "    cooked[\"possible_moves\"] = [\n",
    "        (r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1) if state.k is None or o <= state.k\n",
    "    ]\n",
    "    cooked[\"active_rows_number\"] = sum(o > 0 for o in state.rows)\n",
    "    cooked[\"shortest_row\"] = min((x for x in enumerate(state.rows) if x[1] > 0), key=lambda y: y[1])[0]\n",
    "    cooked[\"longest_row\"] = max((x for x in enumerate(state.rows)), key=lambda y: y[1])[0]\n",
    "    cooked[\"nim_sum\"] = nim_sum(state)\n",
    "\n",
    "    brute_force = list()\n",
    "    for m in cooked[\"possible_moves\"]:\n",
    "        tmp = deepcopy(state)\n",
    "        tmp.nimming(m)\n",
    "        brute_force.append((m, nim_sum(tmp)))\n",
    "    cooked[\"brute_force\"] = brute_force\n",
    "\n",
    "    return cooked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_strategy(state: Nim) -> Nimply:\n",
    "    data = cook_status(state)\n",
    "    return next((bf for bf in data[\"brute_force\"] if bf[1] == 0), random.choice(data[\"brute_force\"]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_random(state: Nim) -> Nimply:\n",
    "    row = random.choice([r for r, c in enumerate(state.rows) if c > 0])\n",
    "    num_objects = random.randint(1, state.rows[row])\n",
    "    return Nimply(row, num_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MATCHES = 100\n",
    "NIM_SIZE = 4\n",
    "\n",
    "\n",
    "def evaluate(strategy1: Callable, strategy2: Callable, nim_size=NIM_SIZE) -> float:\n",
    "    opponent = (strategy1, strategy2)\n",
    "    won = 0\n",
    "\n",
    "    for m in range(NUM_MATCHES):\n",
    "        nim = Nim(nim_size)\n",
    "        player = 0\n",
    "        while nim:\n",
    "            ply = opponent[player](nim)\n",
    "            nim.nimming(ply)\n",
    "            player = 1 - player\n",
    "        if player == 1:\n",
    "            won += 1\n",
    "    return won / NUM_MATCHES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pick_maximum_from_highest_row(state: Nim) -> Nimply:\n",
    "    \"\"\"Pick always the maximum possible number of the highest row\"\"\"\n",
    "    possible_moves = [(r, o) for r, c in enumerate(state.rows)\n",
    "                      for o in range(1, c + 1)]\n",
    "    return Nimply(*max(possible_moves, key=lambda m: (m[0], m[1])))\n",
    "\n",
    "evaluate(pick_maximum_from_highest_row, pure_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pick_minimum_from_lowest_row(state: Nim) -> Nimply:\n",
    "    \"\"\"Pick always the minimum possible number of the lowest row\"\"\"\n",
    "    possible_moves = [(r, o) for r, c in enumerate(state.rows)\n",
    "                      for o in range(1, c + 1)]\n",
    "    return Nimply(*max(possible_moves, key=lambda m: (-m[0], -m[1])))\n",
    "\n",
    "evaluate(pick_minimum_from_lowest_row, pure_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pick_maximum_from_lowest_row(state: Nim) -> Nimply:\n",
    "    \"\"\"Pick always the maximum possible number of the lowest row\"\"\"\n",
    "    possible_moves = [(r, o) for r, c in enumerate(state.rows)\n",
    "                      for o in range(1, c + 1)]\n",
    "    return Nimply(*max(possible_moves, key=lambda m: (m[0], -m[1])))\n",
    "\n",
    "evaluate(pick_maximum_from_lowest_row, pure_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pick_minimum_from_highest_row(state: Nim) -> Nimply:\n",
    "    \"\"\"Pick always the minimum possible number of the highest row\"\"\"\n",
    "    possible_moves = [(r, o) for r, c in enumerate(state.rows)\n",
    "                      for o in range(1, c + 1)]\n",
    "    return Nimply(*max(possible_moves, key=lambda m: (-m[0], m[1])))\n",
    "\n",
    "evaluate(pick_minimum_from_highest_row, pure_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_and_decide(state: Nim) -> Nimply:\n",
    "    if active_rows(state) % 2 == 0:\n",
    "        return pick_maximum_from_highest_row(state)\n",
    "    else:\n",
    "        return pick_minimum_from_lowest_row(state)\n",
    "\n",
    "\n",
    "evaluate(count_and_decide, pure_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_and_decide(state: Nim) -> Nimply:\n",
    "    if active_rows(state) % 2 != 0:\n",
    "        return pick_maximum_from_highest_row(state)\n",
    "    else:\n",
    "        return pick_minimum_from_lowest_row(state)\n",
    "\n",
    "evaluate(count_and_decide, pure_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_and_decide(state: Nim) -> Nimply:\n",
    "    if active_rows(state) % 2 == 0:\n",
    "        return pick_maximum_from_highest_row(state)\n",
    "    else:\n",
    "        return pick_maximum_from_lowest_row(state)\n",
    "\n",
    "\n",
    "evaluate(count_and_decide, pure_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_and_decide(state: Nim) -> Nimply:\n",
    "    if active_rows(state) % 2 != 0:\n",
    "        return pick_maximum_from_highest_row(state)\n",
    "    else:\n",
    "        return pick_maximum_from_lowest_row(state)\n",
    "\n",
    "evaluate(count_and_decide, pure_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_and_decide(state: Nim) -> Nimply:\n",
    "    if active_rows(state) % 2 == 0:\n",
    "        return pick_maximum_from_highest_row(state)\n",
    "    else:\n",
    "        return pick_minimum_from_highest_row(state)\n",
    "\n",
    "\n",
    "evaluate(count_and_decide, pure_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_and_decide(state: Nim) -> Nimply:\n",
    "    if active_rows(state) % 2 != 0:\n",
    "        return pick_maximum_from_highest_row(state)\n",
    "    else:\n",
    "        return pick_minimum_from_highest_row(state)\n",
    "\n",
    "\n",
    "evaluate(count_and_decide, pure_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_and_decide(state: Nim) -> Nimply:\n",
    "    if active_rows(state) % 2 == 0:\n",
    "        return pick_minimum_from_lowest_row(state)\n",
    "    else:\n",
    "        return pick_maximum_from_lowest_row(state)\n",
    "\n",
    "\n",
    "evaluate(count_and_decide, pure_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_and_decide(state: Nim) -> Nimply:\n",
    "    if active_rows(state) % 2 != 0:\n",
    "        return pick_minimum_from_lowest_row(state)\n",
    "    else:\n",
    "        return pick_maximum_from_lowest_row(state)\n",
    "\n",
    "\n",
    "evaluate(count_and_decide, pure_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_and_decide(state: Nim) -> Nimply:\n",
    "    if active_rows(state) % 2 == 0:\n",
    "        return pick_minimum_from_lowest_row(state)\n",
    "    else:\n",
    "        return pick_minimum_from_highest_row(state)\n",
    "\n",
    "\n",
    "evaluate(count_and_decide, pure_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_and_decide(state: Nim) -> Nimply:\n",
    "    if active_rows(state) % 2 != 0:\n",
    "        return pick_minimum_from_lowest_row(state)\n",
    "    else:\n",
    "        return pick_minimum_from_highest_row(state)\n",
    "\n",
    "\n",
    "evaluate(count_and_decide, pure_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_and_decide(state: Nim) -> Nimply:\n",
    "    if active_rows(state) % 2 == 0:\n",
    "        return pick_maximum_from_lowest_row(state)\n",
    "    else:\n",
    "        return pick_minimum_from_highest_row(state)\n",
    "\n",
    "\n",
    "evaluate(count_and_decide, pure_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_and_decide(state: Nim) -> Nimply:\n",
    "    if active_rows(state) % 2 != 0:\n",
    "        return pick_maximum_from_lowest_row(state)\n",
    "    else:\n",
    "        return pick_minimum_from_highest_row(state)\n",
    "\n",
    "\n",
    "evaluate(count_and_decide, pure_random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_all_elements(state: Nim) -> Nimply:\n",
    "    row = random.choice([r for r, c in enumerate(state.rows) if c > 0])\n",
    "    return Nimply(row, state.rows[row])\n",
    "\n",
    "\n",
    "def pick_all_but_one_elements(state: Nim) -> Nimply:\n",
    "    row = random.choice([r for r, c in enumerate(state.rows) if c > 0])\n",
    "    return Nimply(row, state.rows[row] - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def count_and_decide(state: Nim) -> Nimply:\n",
    "    if active_rows(state) % 2 == 0:\n",
    "        return pick_all_but_one_elements(state)\n",
    "    else:\n",
    "        return pick_all_elements(state)\n",
    "\n",
    "print(evaluate(count_and_decide, pure_random))\n",
    "print(evaluate(count_and_decide, optimal_strategy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "Individual = namedtuple(\"Individual\", [\"genome\", \"fitness\"])\n",
    "POPULATION_SIZE = 50\n",
    "NUM_GENERATIONS = 100\n",
    "OFFSPRING_SIZE = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fitness(genome, strategy):\n",
    "    return evaluate(strategy(genome), optimal_strategy)\n",
    "\n",
    "\n",
    "def tournament(population, tournament_size=2):\n",
    "    return max(random.choices(population, k=tournament_size), key=lambda i: i.fitness)\n",
    "\n",
    "\n",
    "def mutation():\n",
    "    return str(random.random())\n",
    "\n",
    "\n",
    "def crossover(g_1, g_2):\n",
    "    g_x = (float(g_1) + float(g_2))/2\n",
    "    return str(g_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_genetic_algorithm(population, strategy):\n",
    "    for generation in range(NUM_GENERATIONS):\n",
    "        offspring = list()\n",
    "        for i in range(OFFSPRING_SIZE):\n",
    "            if random.random() < 0.2:\n",
    "                p = tournament(population)\n",
    "                o = mutation()\n",
    "            else:\n",
    "                # promising genome 1\n",
    "                p1 = tournament(population)\n",
    "                # promising genome 2\n",
    "                p2 = tournament(population)\n",
    "                o = crossover(p1.genome, p2.genome)\n",
    "            f = compute_fitness(o, strategy)\n",
    "            offspring.append(Individual(o, f))\n",
    "\n",
    "        population += offspring\n",
    "        population = sorted(population, key=lambda i: i.fitness, reverse=True)[\n",
    "            :POPULATION_SIZE]\n",
    "\n",
    "        best_so_far = population[0]\n",
    "        if (generation % 5 == 0):\n",
    "            print(\n",
    "                f\"Generation #{generation}\\t\\tGENOME (Probability): {best_so_far.genome}\\tFITNESS: {best_so_far.fitness}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolution(evolvable_strategy):\n",
    "    population = list()\n",
    "    for _ in range(POPULATION_SIZE):\n",
    "        genome = str(random.random())\n",
    "        population.append(Individual(\n",
    "            genome, compute_fitness(genome, evolvable_strategy)))\n",
    "\n",
    "    my_genetic_algorithm(population, evolvable_strategy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation #0\t\tGENOME (Probability): 0.3916160795171203\tFITNESS: 0.33\n",
      "Generation #5\t\tGENOME (Probability): 0.40491509329877645\tFITNESS: 0.38\n",
      "Generation #10\t\tGENOME (Probability): 0.5178968736576\tFITNESS: 0.4\n",
      "Generation #15\t\tGENOME (Probability): 0.5178968736576\tFITNESS: 0.4\n",
      "Generation #20\t\tGENOME (Probability): 0.5178968736576\tFITNESS: 0.4\n",
      "Generation #25\t\tGENOME (Probability): 0.4614059834781882\tFITNESS: 0.41\n",
      "Generation #30\t\tGENOME (Probability): 0.4614059834781882\tFITNESS: 0.41\n",
      "Generation #35\t\tGENOME (Probability): 0.4614059834781882\tFITNESS: 0.41\n",
      "Generation #40\t\tGENOME (Probability): 0.4614059834781882\tFITNESS: 0.41\n",
      "Generation #45\t\tGENOME (Probability): 0.4614059834781882\tFITNESS: 0.41\n",
      "Generation #50\t\tGENOME (Probability): 0.4614059834781882\tFITNESS: 0.41\n",
      "Generation #55\t\tGENOME (Probability): 0.4614059834781882\tFITNESS: 0.41\n",
      "Generation #60\t\tGENOME (Probability): 0.4614059834781882\tFITNESS: 0.41\n",
      "Generation #65\t\tGENOME (Probability): 0.4614059834781882\tFITNESS: 0.41\n",
      "Generation #70\t\tGENOME (Probability): 0.4614059834781882\tFITNESS: 0.41\n",
      "Generation #75\t\tGENOME (Probability): 0.4614059834781882\tFITNESS: 0.41\n",
      "Generation #80\t\tGENOME (Probability): 0.4614059834781882\tFITNESS: 0.41\n",
      "Generation #85\t\tGENOME (Probability): 0.4614059834781882\tFITNESS: 0.41\n",
      "Generation #90\t\tGENOME (Probability): 0.4614059834781882\tFITNESS: 0.41\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[551], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[39mreturn\u001b[39;00m ply\n\u001b[1;32m     10\u001b[0m     \u001b[39mreturn\u001b[39;00m evolvable\n\u001b[0;32m---> 12\u001b[0m evolution(make_strategy)\n",
      "Cell \u001b[0;32mIn[550], line 8\u001b[0m, in \u001b[0;36mevolution\u001b[0;34m(evolvable_strategy)\u001b[0m\n\u001b[1;32m      4\u001b[0m     genome \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(random\u001b[39m.\u001b[39mrandom())\n\u001b[1;32m      5\u001b[0m     population\u001b[39m.\u001b[39mappend(Individual(\n\u001b[1;32m      6\u001b[0m         genome, compute_fitness(genome, evolvable_strategy)))\n\u001b[0;32m----> 8\u001b[0m my_genetic_algorithm(population, evolvable_strategy)\n",
      "Cell \u001b[0;32mIn[549], line 14\u001b[0m, in \u001b[0;36mmy_genetic_algorithm\u001b[0;34m(population, strategy)\u001b[0m\n\u001b[1;32m     12\u001b[0m         p2 \u001b[39m=\u001b[39m tournament(population)\n\u001b[1;32m     13\u001b[0m         o \u001b[39m=\u001b[39m crossover(p1\u001b[39m.\u001b[39mgenome, p2\u001b[39m.\u001b[39mgenome)\n\u001b[0;32m---> 14\u001b[0m     f \u001b[39m=\u001b[39m compute_fitness(o, strategy)\n\u001b[1;32m     15\u001b[0m     offspring\u001b[39m.\u001b[39mappend(Individual(o, f))\n\u001b[1;32m     17\u001b[0m population \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m offspring\n",
      "Cell \u001b[0;32mIn[548], line 2\u001b[0m, in \u001b[0;36mcompute_fitness\u001b[0;34m(genome, strategy)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_fitness\u001b[39m(genome, strategy):\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mreturn\u001b[39;00m evaluate(strategy(genome), optimal_strategy)\n",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(strategy1, strategy2, nim_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m player \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[39mwhile\u001b[39;00m nim:\n\u001b[0;32m---> 13\u001b[0m     ply \u001b[39m=\u001b[39m opponent[player](nim)\n\u001b[1;32m     14\u001b[0m     nim\u001b[39m.\u001b[39mnimming(ply)\n\u001b[1;32m     15\u001b[0m     player \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m player\n",
      "Cell \u001b[0;32mIn[551], line 7\u001b[0m, in \u001b[0;36mmake_strategy.<locals>.evolvable\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      5\u001b[0m     ply \u001b[39m=\u001b[39m pick_all_elements(state)\n\u001b[1;32m      6\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     ply \u001b[39m=\u001b[39m pick_all_but_one_elements(state)\n\u001b[1;32m      8\u001b[0m \u001b[39mreturn\u001b[39;00m ply\n",
      "Cell \u001b[0;32mIn[475], line 7\u001b[0m, in \u001b[0;36mpick_all_but_one_elements\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpick_all_but_one_elements\u001b[39m(state: Nim) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Nimply:\n\u001b[0;32m----> 7\u001b[0m     row \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39;49mchoice([r \u001b[39mfor\u001b[39;49;00m r, c \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(state\u001b[39m.\u001b[39;49mrows) \u001b[39mif\u001b[39;49;00m c \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m])\n\u001b[1;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m Nimply(row, state\u001b[39m.\u001b[39mrows[row] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/random.py:378\u001b[0m, in \u001b[0;36mRandom.choice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39m\"\"\"Choose a random element from a non-empty sequence.\"\"\"\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[39m# raises IndexError if seq is empty\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m \u001b[39mreturn\u001b[39;00m seq[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_randbelow(\u001b[39mlen\u001b[39;49m(seq))]\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.9/Frameworks/Python.framework/Versions/3.10/lib/python3.10/random.py:245\u001b[0m, in \u001b[0;36mRandom._randbelow_with_getrandbits\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m    244\u001b[0m getrandbits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetrandbits\n\u001b[0;32m--> 245\u001b[0m k \u001b[39m=\u001b[39m n\u001b[39m.\u001b[39;49mbit_length()  \u001b[39m# don't use (n-1) here because n can be 1\u001b[39;00m\n\u001b[1;32m    246\u001b[0m r \u001b[39m=\u001b[39m getrandbits(k)  \u001b[39m# 0 <= r < 2**k\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[39mwhile\u001b[39;00m r \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m n:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def make_strategy(genome: str) -> Callable:\n",
    "    def evolvable(state: Nim) -> Nimply:\n",
    "\n",
    "        if random.random() < float(genome):\n",
    "            ply = pick_all_elements(state)\n",
    "        else:\n",
    "            ply = pick_all_but_one_elements(state)\n",
    "        return ply\n",
    "\n",
    "    return evolvable\n",
    "\n",
    "evolution(make_strategy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.3\n",
    "MiniMax agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def minmax(state: Nim, maximizing_player: bool, alpha = -1, beta = 1):\n",
    "    if not state:\n",
    "        return -1 if maximizing_player else 1\n",
    "    \n",
    "    data = cook_status(state)\n",
    "    possible_next_states = []\n",
    "\n",
    "    for ply in data['possible_moves']:\n",
    "        tmp_state = deepcopy(state)\n",
    "        tmp_state.nimming(ply)\n",
    "        possible_next_states.append(tmp_state)\n",
    "    \n",
    "    if maximizing_player:\n",
    "        best_val = -math.inf\n",
    "\n",
    "        for next_state in possible_next_states:\n",
    "            val = minmax(next_state, not maximizing_player, alpha, beta)\n",
    "            best_val = max(best_val, val)\n",
    "            alpha = max(alpha, best_val)\n",
    "\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return best_val\n",
    "    else:\n",
    "        best_val = math.inf\n",
    "        next_state = deepcopy(state)\n",
    "        ply = optimal_strategy(next_state)\n",
    "        next_state.nimming(ply)\n",
    "        \n",
    "        val = minmax(next_state, not maximizing_player, alpha, beta)\n",
    "        best_val = min(best_val, val)\n",
    "        beta = min(beta, best_val)\n",
    "\n",
    "        return best_val\n",
    "\n",
    "def minmax_strategy(state: Nim) -> Nimply:\n",
    "    data = cook_status(state)\n",
    "\n",
    "    for ply in data['possible_moves']:\n",
    "        tmp_state = deepcopy(state)\n",
    "        tmp_state.nimming(ply)\n",
    "\n",
    "        score = minmax(tmp_state, maximizing_player = True)\n",
    "        if score > 0:\n",
    "            break\n",
    "    return ply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(minmax_strategy, pure_random, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(minmax_strategy, pure_random, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(minmax_strategy, optimal_strategy, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[584], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluate(minmax_strategy, optimal_strategy, \u001b[39m5\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(strategy1, strategy2, nim_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m player \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[39mwhile\u001b[39;00m nim:\n\u001b[0;32m---> 13\u001b[0m     ply \u001b[39m=\u001b[39m opponent[player](nim)\n\u001b[1;32m     14\u001b[0m     nim\u001b[39m.\u001b[39mnimming(ply)\n\u001b[1;32m     15\u001b[0m     player \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m player\n",
      "Cell \u001b[0;32mIn[580], line 45\u001b[0m, in \u001b[0;36mminmax_strategy\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     42\u001b[0m tmp_state \u001b[39m=\u001b[39m deepcopy(state)\n\u001b[1;32m     43\u001b[0m tmp_state\u001b[39m.\u001b[39mnimming(ply)\n\u001b[0;32m---> 45\u001b[0m score \u001b[39m=\u001b[39m minmax(tmp_state, maximizing_player \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m score \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     47\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[580], line 19\u001b[0m, in \u001b[0;36mminmax\u001b[0;34m(state, maximizing_player, alpha, beta)\u001b[0m\n\u001b[1;32m     16\u001b[0m best_val \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mmath\u001b[39m.\u001b[39minf\n\u001b[1;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m next_state \u001b[39min\u001b[39;00m possible_next_states:\n\u001b[0;32m---> 19\u001b[0m     val \u001b[39m=\u001b[39m minmax(next_state, \u001b[39mnot\u001b[39;49;00m maximizing_player, alpha, beta)\n\u001b[1;32m     20\u001b[0m     best_val \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(best_val, val)\n\u001b[1;32m     21\u001b[0m     alpha \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(alpha, best_val)\n",
      "Cell \u001b[0;32mIn[580], line 32\u001b[0m, in \u001b[0;36mminmax\u001b[0;34m(state, maximizing_player, alpha, beta)\u001b[0m\n\u001b[1;32m     29\u001b[0m ply \u001b[39m=\u001b[39m optimal_strategy(next_state)\n\u001b[1;32m     30\u001b[0m next_state\u001b[39m.\u001b[39mnimming(ply)\n\u001b[0;32m---> 32\u001b[0m val \u001b[39m=\u001b[39m minmax(next_state, \u001b[39mnot\u001b[39;49;00m maximizing_player, alpha, beta)\n\u001b[1;32m     33\u001b[0m best_val \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(best_val, val)\n\u001b[1;32m     34\u001b[0m beta \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(beta, best_val)\n",
      "Cell \u001b[0;32mIn[580], line 19\u001b[0m, in \u001b[0;36mminmax\u001b[0;34m(state, maximizing_player, alpha, beta)\u001b[0m\n\u001b[1;32m     16\u001b[0m best_val \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mmath\u001b[39m.\u001b[39minf\n\u001b[1;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m next_state \u001b[39min\u001b[39;00m possible_next_states:\n\u001b[0;32m---> 19\u001b[0m     val \u001b[39m=\u001b[39m minmax(next_state, \u001b[39mnot\u001b[39;49;00m maximizing_player, alpha, beta)\n\u001b[1;32m     20\u001b[0m     best_val \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(best_val, val)\n\u001b[1;32m     21\u001b[0m     alpha \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(alpha, best_val)\n",
      "Cell \u001b[0;32mIn[580], line 32\u001b[0m, in \u001b[0;36mminmax\u001b[0;34m(state, maximizing_player, alpha, beta)\u001b[0m\n\u001b[1;32m     29\u001b[0m ply \u001b[39m=\u001b[39m optimal_strategy(next_state)\n\u001b[1;32m     30\u001b[0m next_state\u001b[39m.\u001b[39mnimming(ply)\n\u001b[0;32m---> 32\u001b[0m val \u001b[39m=\u001b[39m minmax(next_state, \u001b[39mnot\u001b[39;49;00m maximizing_player, alpha, beta)\n\u001b[1;32m     33\u001b[0m best_val \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(best_val, val)\n\u001b[1;32m     34\u001b[0m beta \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(beta, best_val)\n",
      "    \u001b[0;31m[... skipping similar frames: minmax at line 19 (4 times), minmax at line 32 (3 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[580], line 32\u001b[0m, in \u001b[0;36mminmax\u001b[0;34m(state, maximizing_player, alpha, beta)\u001b[0m\n\u001b[1;32m     29\u001b[0m ply \u001b[39m=\u001b[39m optimal_strategy(next_state)\n\u001b[1;32m     30\u001b[0m next_state\u001b[39m.\u001b[39mnimming(ply)\n\u001b[0;32m---> 32\u001b[0m val \u001b[39m=\u001b[39m minmax(next_state, \u001b[39mnot\u001b[39;49;00m maximizing_player, alpha, beta)\n\u001b[1;32m     33\u001b[0m best_val \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(best_val, val)\n\u001b[1;32m     34\u001b[0m beta \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(beta, best_val)\n",
      "Cell \u001b[0;32mIn[580], line 19\u001b[0m, in \u001b[0;36mminmax\u001b[0;34m(state, maximizing_player, alpha, beta)\u001b[0m\n\u001b[1;32m     16\u001b[0m best_val \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mmath\u001b[39m.\u001b[39minf\n\u001b[1;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m next_state \u001b[39min\u001b[39;00m possible_next_states:\n\u001b[0;32m---> 19\u001b[0m     val \u001b[39m=\u001b[39m minmax(next_state, \u001b[39mnot\u001b[39;49;00m maximizing_player, alpha, beta)\n\u001b[1;32m     20\u001b[0m     best_val \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(best_val, val)\n\u001b[1;32m     21\u001b[0m     alpha \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(alpha, best_val)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate(minmax_strategy, optimal_strategy, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.4\n",
    "RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QL_Agent():\n",
    "    q = {}\n",
    "    previous_state = previous_action = None\n",
    "    WIN_REWARD, LOSS_REWARD = 1, -1\n",
    "\n",
    "    def __init__(self, state, k = None, epsilon = 1, learning_rate = 1, discount_factor = 1) -> None:\n",
    "        #q is a function f: State x Action -> R and is internally represented as a Map.\n",
    "\n",
    "        #alpha is the learning rate and determines to what extent the newly acquired \n",
    "        #information will override the old information\n",
    "\n",
    "        #gamma is the discount rate and determines the importance of future rewards\n",
    "\n",
    "        #epsilon serves as the exploration rate and determines the probability \n",
    "        #that the agent, in the learning process, will randomly select an action\n",
    "\n",
    "        self.epsilon = epsilon                      # epsilon   -> the higher epsilon,  the more random I act\n",
    "        self.learning_rate = learning_rate          # alpha     -> the higher alpha,    the more I replace \"q\"\n",
    "        self.discount_factor = discount_factor      # gamma     -> the higher gamma,    the more I favor long-term reward\n",
    "        # as I get closer and closer to the deadline, my preference for near-term reward should increase, \n",
    "        # which means my gamma should decrease.\n",
    "\n",
    "    def makeKey(self, state):\n",
    "        possActions = list(self.getActions(state))\n",
    "        someAction = possActions[0]\n",
    "\n",
    "        # generating Q Table\n",
    "        if (tuple(state), someAction) not in self.q:\n",
    "            for i in possActions:\n",
    "                self.q[(tuple(state), i)] = np.random.uniform(0.0, 0.01)\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        '''returns True if the state is terminal'''\n",
    "        return sum(state) == 0\n",
    "\n",
    "    def getActions(self, state):\n",
    "        '''returns a list of possible actions for a given state'''\n",
    "        if self.is_terminal(state):\n",
    "            return [None]\n",
    "\n",
    "        possible_actions = []\n",
    "        for row, num_objects in enumerate(state):\n",
    "            for remaining in range(num_objects):\n",
    "                possible_actions.append((row, num_objects - remaining))\n",
    "        return possible_actions\n",
    "\n",
    "\n",
    "    def policy(self, state):\n",
    "        '''Policy\n",
    "        This function takes a state and chooses the action for that state that will lead to the maximum reward'''\n",
    "        possActions = list(self.getActions(state))\n",
    "\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Highest reward -> Low exploration rate\n",
    "            q_values = [self.q[(tuple(state),i)] for i in possActions]\n",
    "            return possActions[np.argmax(q_values)]\n",
    "        else:\n",
    "            # Random -> High exploration rate\n",
    "            chosen_action_idx = np.random.randint(0, len(possActions))\n",
    "            return possActions[chosen_action_idx]\n",
    "\n",
    "    # Updates the Q-table as specified by the standard Q-learning algorithm\n",
    "    def update_q(self, state):\n",
    "        if self.is_terminal(state):\n",
    "            self.q[(tuple(self.previous_state), self.previous_action)] += \\\n",
    "                self.learning_rate * (self.LOSS_REWARD - self.q[(tuple(self.previous_state), self.previous_action)])\n",
    "            \n",
    "            current_action = self.previous_state = self.previous_action = None\n",
    "        else:\n",
    "            self.makeKey(state)\n",
    "            current_action = self.policy(state)\n",
    "\n",
    "            if self.previous_action is not None:\n",
    "                next_state = state.copy()\n",
    "                next_state[current_action[0]] -= current_action[1]\n",
    "\n",
    "                reward = self.WIN_REWARD if self.is_terminal(next_state) else 0\n",
    "                maxQ = max(self.q[(tuple(state), a)] for a in self.getActions(state))\n",
    "                self.q[(tuple(self.previous_state), self.previous_action)] += \\\n",
    "                    self.learning_rate * (reward + self.discount_factor * maxQ - \\\n",
    "                        self.q[(tuple(self.previous_state), self.previous_action)])\n",
    "\n",
    "            self.previous_state, self.previous_action = tuple(state),current_action\n",
    "        return current_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_play(opponent_strategy: Callable, nim_dim = 4):\n",
    "    losses = 0\n",
    "    wins = 0\n",
    "    nGames = 10000\n",
    "\n",
    "    for i in np.arange(nGames):\n",
    "        currState = Nim(nim_dim)                        # Reset game\n",
    "        agent = QL_Agent(currState)                     # Reset Agent \n",
    "\n",
    "        while True:\n",
    "            # Opponent plays\n",
    "            opponent_play = opponent_strategy(currState)\n",
    "            currState.nimming(opponent_play) \n",
    "\n",
    "            action_p1 = agent.update_q(currState._rows) \n",
    "\n",
    "            if(action_p1 is not None):\n",
    "                currState.nimming(Nimply(action_p1[0], action_p1[1]))\n",
    "\n",
    "            if action_p1 is None:\n",
    "                # Player can't do any actions -> LOSS\n",
    "                losses += 1\n",
    "                break\n",
    "            elif currState.game_over():\n",
    "                # Player reached gameover state -> WIN\n",
    "                wins += 1\n",
    "                break  \n",
    "\n",
    "    print(f\"Games: {nGames} Wins: {wins} Losses: {losses} => winrate: {wins/(wins+losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Games: 10000 Wins: 9465 Losses: 535 => winrate: 0.9465\n"
     ]
    }
   ],
   "source": [
    "Q_play(pure_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Games: 10000 Wins: 9317 Losses: 683 => winrate: 0.9317\n"
     ]
    }
   ],
   "source": [
    "Q_play(optimal_strategy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
